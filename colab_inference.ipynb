{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Aa-nRCzPVdF"
      },
      "source": [
        "# IndicTrans2 HF Inference\n",
        "\n",
        "We provide an example notebook on how to use our IndicTrans2 models which were originally trained with the fairseq to HuggingFace transformers for inference purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfsv02IeP2It"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Please run the cells below to install the necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qKcYlUZYGLrt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'IndicTrans2'...\n",
            "remote: Enumerating objects: 631, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 631 (delta 87), reused 77 (delta 70), pack-reused 517\u001b[K\n",
            "Receiving objects: 100% (631/631), 4.09 MiB | 8.55 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U3vs7FkIGSxK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/jeyam/Programming/Hackathons/Indic_Language_Support/IndicTrans2/huggingface_interface\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd IndicTrans2/huggingface_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ddkRAXQ2Git0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: 4.33.2 not found\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ModuleNotFoundError: No module named 'nltk'\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: scipy in /Users/jeyam/miniconda3/lib/python3.11/site-packages (1.11.3)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from scipy) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: psutil in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from accelerate) (2.1.0)\n",
            "Collecting huggingface-hub (from accelerate)\n",
            "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting safetensors>=0.3.1 (from accelerate)\n",
            "  Downloading safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: filelock in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from datasets) (3.12.4)\n",
            "Collecting pyarrow>=8.0.0 (from datasets)\n",
            "  Downloading pyarrow-15.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from datasets) (2.1.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl (387 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.7/387.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-15.0.0-cp311-cp311-macosx_11_0_arm64.whl (24.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
            "Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
            "Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, safetensors, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, yarl, multiprocess, huggingface-hub, bitsandbytes, aiosignal, aiohttp, accelerate, datasets\n",
            "Successfully installed accelerate-0.26.1 aiohttp-3.9.3 aiosignal-1.3.1 bitsandbytes-0.42.0 datasets-2.16.1 dill-0.3.7 frozenlist-1.4.1 huggingface-hub-0.20.3 multidict-6.0.5 multiprocess-0.70.15 pyarrow-15.0.0 pyarrow-hotfix-0.6 safetensors-0.4.2 xxhash-3.4.1 yarl-1.9.4\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Cloning into 'IndicTransTokenizer'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 103 (delta 45), reused 81 (delta 26), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 3.85 MiB | 3.63 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n",
            "/Users/jeyam/Programming/Hackathons/Indic_Language_Support/IndicTrans2/huggingface_interface/IndicTransTokenizer\n",
            "Obtaining file:///Users/jeyam/Programming/Hackathons/Indic_Language_Support/IndicTrans2/huggingface_interface/IndicTransTokenizer\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library (from IndicTransTokenizer==0.1.1)\n",
            "  Cloning https://github.com/VarunGumma/indic_nlp_library to /private/var/folders/m4/817lmlzx26df805ztf8tvs7r0000gn/T/pip-install-zcpmr5zu/indic-nlp-library-it2_ee5e6ad38125419cb328462d8c152076\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/VarunGumma/indic_nlp_library /private/var/folders/m4/817lmlzx26df805ztf8tvs7r0000gn/T/pip-install-zcpmr5zu/indic-nlp-library-it2_ee5e6ad38125419cb328462d8c152076\n",
            "  Resolved https://github.com/VarunGumma/indic_nlp_library to commit 18e85551b0b331a65d11acf869afd4cfa615bc4c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting setuptools==68.2.2 (from IndicTransTokenizer==0.1.1)\n",
            "  Downloading setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: torch in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from IndicTransTokenizer==0.1.1) (2.1.0)\n",
            "Requirement already satisfied: sacremoses in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from IndicTransTokenizer==0.1.1) (0.1.1)\n",
            "Requirement already satisfied: sentencepiece in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from IndicTransTokenizer==0.1.1) (0.1.99)\n",
            "Collecting transformers (from IndicTransTokenizer==0.1.1)\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-argparse (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx_rtd_theme (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: pandas in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1) (2.1.1)\n",
            "Requirement already satisfied: numpy in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1) (1.25.2)\n",
            "Requirement already satisfied: regex in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from sacremoses->IndicTransTokenizer==0.1.1) (2023.12.25)\n",
            "Requirement already satisfied: click in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from sacremoses->IndicTransTokenizer==0.1.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from sacremoses->IndicTransTokenizer==0.1.1) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from sacremoses->IndicTransTokenizer==0.1.1) (4.65.0)\n",
            "Requirement already satisfied: filelock in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch->IndicTransTokenizer==0.1.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch->IndicTransTokenizer==0.1.1) (4.7.1)\n",
            "Requirement already satisfied: sympy in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch->IndicTransTokenizer==0.1.1) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch->IndicTransTokenizer==0.1.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch->IndicTransTokenizer==0.1.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from torch->IndicTransTokenizer==0.1.1) (2023.9.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from transformers->IndicTransTokenizer==0.1.1) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from transformers->IndicTransTokenizer==0.1.1) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from transformers->IndicTransTokenizer==0.1.1) (6.0)\n",
            "Requirement already satisfied: requests in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from transformers->IndicTransTokenizer==0.1.1) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers->IndicTransTokenizer==0.1.1)\n",
            "  Downloading tokenizers-0.15.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from transformers->IndicTransTokenizer==0.1.1) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from jinja2->torch->IndicTransTokenizer==0.1.1) (2.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1) (2022.7)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from requests->transformers->IndicTransTokenizer==0.1.1) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from requests->transformers->IndicTransTokenizer==0.1.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from requests->transformers->IndicTransTokenizer==0.1.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from requests->transformers->IndicTransTokenizer==0.1.1) (2023.7.22)\n",
            "Collecting sphinx>=1.2.0 (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinx-7.2.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting docutils<0.21 (from sphinx_rtd_theme->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx_rtd_theme->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from sympy->torch->IndicTransTokenizer==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1) (1.16.0)\n",
            "Collecting sphinxcontrib-applehelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-devhelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-jsmath (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sphinxcontrib-qthelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: Pygments>=2.14 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1) (2.15.1)\n",
            "Collecting snowballstemmer>=2.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: babel>=2.9 in /Users/jeyam/miniconda3/lib/python3.11/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1) (2.11.0)\n",
            "Collecting alabaster<0.8,>=0.7 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting imagesize>=1.3 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.1)\n",
            "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.1-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sphinx-7.2.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
            "Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: indic-nlp-library-IT2\n",
            "  Building wheel for indic-nlp-library-IT2 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for indic-nlp-library-IT2: filename=indic_nlp_library_IT2-0.0.1-py3-none-any.whl size=56240 sha256=c6ade32abd2e5d190707c8253930a6679dc360917c11d03e4ce54adc1033ce6e\n",
            "  Stored in directory: /private/var/folders/m4/817lmlzx26df805ztf8tvs7r0000gn/T/pip-ephem-wheel-cache-e1bfo5c7/wheels/51/85/db/bca7617519176478fea30c6a18f12792792a5fd72be3f91b17\n",
            "Successfully built indic-nlp-library-IT2\n",
            "Installing collected packages: snowballstemmer, morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, setuptools, imagesize, docutils, alabaster, sphinx, tokenizers, sphinxcontrib-jquery, sphinx-argparse, transformers, sphinx_rtd_theme, indic-nlp-library-IT2, IndicTransTokenizer\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.8.0\n",
            "    Uninstalling setuptools-67.8.0:\n",
            "      Successfully uninstalled setuptools-67.8.0\n",
            "  Running setup.py develop for IndicTransTokenizer\n",
            "Successfully installed IndicTransTokenizer-0.1.1 alabaster-0.7.16 docutils-0.20.1 imagesize-1.4.1 indic-nlp-library-IT2-0.0.1 morfessor-2.0.6 setuptools-68.2.2 snowballstemmer-2.2.0 sphinx-7.2.6 sphinx-argparse-0.4.0 sphinx_rtd_theme-2.0.0 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10 tokenizers-0.15.1 transformers-4.37.2\n",
            "/Users/jeyam/Programming/Hackathons/Indic_Language_Support/IndicTrans2/huggingface_interface\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransTokenizer\n",
        "%cd IndicTransTokenizer\n",
        "!python3 -m pip install --editable ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjN7ub1tO33H"
      },
      "source": [
        "**IMPORTANT : Restart your run-time first and then run the cells below.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SLBIw6rQB-0"
      },
      "source": [
        "## Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fYczM2U6G1Zv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
        "from IndicTransTokenizer import IndicProcessor, IndicTransTokenizer\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "quantization = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xj1WCNjuHG-d"
      },
      "outputs": [],
      "source": [
        "def initialize_model_and_tokenizer(ckpt_dir, direction, quantization):\n",
        "    if quantization == \"4-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    elif quantization == \"8-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_use_double_quant=True,\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    else:\n",
        "        qconfig = None\n",
        "\n",
        "    tokenizer = IndicTransTokenizer(direction=direction)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ckpt_dir,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        quantization_config=qconfig,\n",
        "    )\n",
        "\n",
        "    if qconfig == None:\n",
        "        model = model.to(DEVICE)\n",
        "        if DEVICE == \"cuda\":\n",
        "            model.half()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    translations = []\n",
        "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
        "        batch = input_sentences[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Preprocess the batch and extract entity mappings\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "        # Tokenize the batch and generate input encodings\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            src=True,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate translations using the model\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                use_cache=True,\n",
        "                min_length=0,\n",
        "                max_length=256,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into text\n",
        "        generated_tokens = tokenizer.batch_decode(generated_tokens.detach().cpu().tolist(), src=False)\n",
        "\n",
        "        # Postprocess the translations, including entity replacement\n",
        "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
        "\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return translations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erNCuZTEMt49"
      },
      "source": [
        "### English to Indic Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OG3Bw-sHnf3",
        "outputId": "a204f50e-9456-4fb1-900a-e60680b97b99"
      },
      "outputs": [],
      "source": [
        "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"  # ai4bharat/indictrans2-en-indic-dist-200M\n",
        "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, \"en-indic\", quantization)\n",
        "\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "\n",
        "\n",
        "# flush the models to free the GPU memory\n",
        "# del en_indic_tokenizer, en_indic_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "eng_Latn - tam_Taml\n",
            "eng_Latn: When I was young, I used to go to the park every day.\n",
            "tam_Taml: நான் இளமையாக இருந்தபோது, தினமும் பூங்காவுக்குச் செல்வது வழக்கம்.\n",
            "eng_Latn: He has many old books, which he inherited from his ancestors.\n",
            "tam_Taml: அவர் தனது மூதாதையர்களிடமிருந்து மரபுரிமையாகப் பெற்ற பல பழைய புத்தகங்கள் அவரிடம் உள்ளன.\n",
            "eng_Latn: I can't figure out how to solve my problem.\n",
            "tam_Taml: என் பிரச்சினையை எப்படி தீர்ப்பது என்று எனக்கு புரியவில்லை.\n",
            "eng_Latn: She is very hardworking and intelligent, which is why she got all the good marks.\n",
            "tam_Taml: அவள் மிகவும் கடின உழைப்பாளி மற்றும் புத்திசாலி, அதனால்தான் அவளுக்கு அனைத்து நல்ல மதிப்பெண்களும் கிடைத்தன.\n",
            "eng_Latn: We watched a new movie last week, which was very inspiring.\n",
            "tam_Taml: கடந்த வாரம் ஒரு புதிய திரைப்படத்தைப் பார்த்தோம், அது மிகவும் ஊக்கமளித்தது.\n",
            "eng_Latn: If you had met me at that time, we would have gone out to eat.\n",
            "tam_Taml: அந்த நேரத்தில் நீங்கள் என்னை சந்தித்திருந்தால், நாங்கள் சாப்பிட வெளியே சென்றிருப்போம்.\n",
            "eng_Latn: She went to the market with her sister to buy a new sari.\n",
            "tam_Taml: அவர் தனது சகோதரியுடன் ஒரு புதிய புடவை வாங்க சந்தைக்குச் சென்றார்.\n",
            "eng_Latn: Raj told me that he is going to his grandmother's house next month.\n",
            "tam_Taml: அடுத்த மாதம் தனது பாட்டியின் வீட்டிற்குச் செல்வதாக ராஜ் என்னிடம் கூறினார்.\n",
            "eng_Latn: All the kids were having fun at the party and were eating lots of sweets.\n",
            "tam_Taml: அனைத்து குழந்தைகளும் விருந்தில் வேடிக்கையாக இருந்தனர் மற்றும் நிறைய இனிப்புகள் சாப்பிட்டனர்.\n",
            "eng_Latn: My friend has invited me to his birthday party, and I will give him a gift.\n",
            "tam_Taml: என் நண்பர் தனது பிறந்தநாள் விழாவிற்கு என்னை அழைத்துள்ளார், நான் அவருக்கு ஒரு பரிசை வழங்குவேன்.\n",
            "eng_Latn: I am going to the market to buy some vegetables and fruits.\n",
            "tam_Taml: நான் சில காய்கறிகள் மற்றும் பழங்களை வாங்க சந்தைக்குச் செல்கிறேன்.\n"
          ]
        }
      ],
      "source": [
        "en_sents = [\n",
        "    \"When I was young, I used to go to the park every day.\",\n",
        "    \"He has many old books, which he inherited from his ancestors.\",\n",
        "    \"I can't figure out how to solve my problem.\",\n",
        "    \"She is very hardworking and intelligent, which is why she got all the good marks.\",\n",
        "    \"We watched a new movie last week, which was very inspiring.\",\n",
        "    \"If you had met me at that time, we would have gone out to eat.\",\n",
        "    \"She went to the market with her sister to buy a new sari.\",\n",
        "    \"Raj told me that he is going to his grandmother's house next month.\",\n",
        "    \"All the kids were having fun at the party and were eating lots of sweets.\",\n",
        "    \"My friend has invited me to his birthday party, and I will give him a gift.\",\n",
        "    \"I am going to the market to buy some vegetables and fruits.\",\n",
        "]\n",
        "\n",
        "src_lang, tgt_lang = \"eng_Latn\", \"tam_Taml\"\n",
        "hi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(en_sents, hi_translations):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM_1pbPtMpV9"
      },
      "source": [
        "### Indic to English Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLCEWJKvGG9I",
        "outputId": "ab9d8726-67c7-490b-ecb3-208df1c0f741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "hin_Deva - eng_Latn\n",
            "hin_Deva: जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\n",
            "eng_Latn: When I was young, I used to go to the park every day.\n",
            "hin_Deva: उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\n",
            "eng_Latn: She has a lot of old books, which she inherited from her grandparents.\n",
            "hin_Deva: मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\n",
            "eng_Latn: I don't know how to find a solution to my problem.\n",
            "hin_Deva: वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\n",
            "eng_Latn: He is very hardworking and understanding, so he got all the good marks.\n",
            "hin_Deva: हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\n",
            "eng_Latn: We saw a new movie last week that was very inspiring.\n",
            "hin_Deva: अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\n",
            "eng_Latn: If you'd given me a pass at that time, we'd have gone out to eat.\n",
            "hin_Deva: वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\n",
            "eng_Latn: She had gone to the market with her sister so that she could buy a new sari.\n",
            "hin_Deva: राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\n",
            "eng_Latn: Raj told me that he was going to his grandmother's house next month.\n",
            "hin_Deva: सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\n",
            "eng_Latn: All the children were having fun at the party and eating a lot of sweets.\n",
            "hin_Deva: मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\n",
            "eng_Latn: My friend has invited me to her birthday party, and I'll give her a present.\n"
          ]
        }
      ],
      "source": [
        "indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-1B\"  # ai4bharat/indictrans2-indic-en-dist-200M\n",
        "indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, \"indic-en\", \"\")\n",
        "\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "hi_sents = [\n",
        "    \"जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\",\n",
        "    \"उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\",\n",
        "    \"मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\",\n",
        "    \"वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\",\n",
        "    \"हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\",\n",
        "    \"अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\",\n",
        "    \"वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\",\n",
        "    \"राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\",\n",
        "    \"सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\",\n",
        "    \"मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\",\n",
        "]\n",
        "src_lang, tgt_lang = \"hin_Deva\", \"eng_Latn\"\n",
        "en_translations = batch_translate(hi_sents, src_lang, tgt_lang, indic_en_model, indic_en_tokenizer, ip)\n",
        "\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(hi_sents, en_translations):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")\n",
        "\n",
        "# flush the models to free the GPU memory\n",
        "del indic_en_tokenizer, indic_en_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VCAkyKBGtnV"
      },
      "source": [
        "### Indic to Indic Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7TxTTCoKjti",
        "outputId": "df1a750b-0f32-478d-cfc9-e445f669f3ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "hin_Deva - mar_Deva\n",
            "hin_Deva: जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\n",
            "mar_Deva: मी लहान होतो तेव्हा मी दररोज उद्यानाला जायचे.\n",
            "hin_Deva: उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\n",
            "mar_Deva: तिच्याकडे बरीच जुनी पुस्तके आहेत, जी तिला तिच्या आजोबांकडून वारशाने मिळाली आहेत.\n",
            "hin_Deva: मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\n",
            "mar_Deva: माझ्या समस्येवर तोडगा कसा काढायचा हे मला समजत नाही.\n",
            "hin_Deva: वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\n",
            "mar_Deva: तो खूप मेहनती आणि बुद्धिमान आहे, त्यामुळे त्याला सर्व चांगले गुण मिळाले.\n",
            "hin_Deva: हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\n",
            "mar_Deva: आम्ही गेल्या आठवड्यात एक नवीन चित्रपट पाहिला जो खूप प्रेरणादायी होता.\n",
            "hin_Deva: अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\n",
            "mar_Deva: जर तुम्हाला त्या वेळी मला पास मिळाला तर आम्ही बाहेर जेवायला जाऊ.\n",
            "hin_Deva: वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\n",
            "mar_Deva: ती तिच्या बहिणीसोबत बाजारात गेली होती जेणेकरून ती नवीन साडी खरेदी करू शकेल.\n",
            "hin_Deva: राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\n",
            "mar_Deva: राजने मला सांगितले की तो पुढच्या महिन्यात त्याच्या आजीच्या घरी जात आहे.\n",
            "hin_Deva: सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\n",
            "mar_Deva: सर्व मुले पार्टीचा आनंद घेत होती आणि भरपूर मिठाई खात होती.\n",
            "hin_Deva: मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\n",
            "mar_Deva: माझ्या मित्राने मला त्याच्या वाढदिवसाच्या मेजवानीसाठी आमंत्रित केले आहे आणि मी त्याला भेटवस्तू देईन.\n"
          ]
        }
      ],
      "source": [
        "indic_indic_ckpt_dir = \"ai4bharat/indictrans2-indic-indic-1B\"  # ai4bharat/indictrans2-indic-indic-dist-320M\n",
        "indic_indic_tokenizer, indic_indic_model = initialize_model_and_tokenizer(indic_indic_ckpt_dir, \"indic-indic\", quantization)\n",
        "\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "hi_sents = [\n",
        "    \"जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\",\n",
        "    \"उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\",\n",
        "    \"मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\",\n",
        "    \"वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\",\n",
        "    \"हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\",\n",
        "    \"अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\",\n",
        "    \"वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\",\n",
        "    \"राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\",\n",
        "    \"सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\",\n",
        "    \"मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\",\n",
        "]\n",
        "src_lang, tgt_lang = \"hin_Deva\", \"mar_Deva\"\n",
        "mr_translations = batch_translate(hi_sents, src_lang, tgt_lang, indic_indic_model, indic_indic_tokenizer, ip)\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(hi_sents, mr_translations):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")\n",
        "\n",
        "# flush the models to free the GPU memory\n",
        "del indic_indic_tokenizer, indic_indic_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyxXpt--Ma6n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
